{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4308f4e53c34c3e86de36257a1939f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "complete\\sceptr-default-autoencoder-0\\kfold-0; Progress: 56/125:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a7e46d71d142d0aaa92cc5e06831b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "complete\\sceptr-default-autoencoder-0\\kfold-1; Progress: 57/125:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/rcwyuen/OneDrive/Studies/UCL/Publications/TCR-Embeddings\")\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import training.models as model\n",
    "import reduction.reduction as reducer\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "use_cuda = False\n",
    "data_src = \"results/\"\n",
    "src = Path(\"C:/Users/rcwyuen/OneDrive/Studies/UCL/Publications/TCR-Embeddings\")\n",
    "\n",
    "def read_kfold_set(pth, label):\n",
    "    with open(pth, \"r\") as f:\n",
    "        kf = f.readlines()\n",
    "        assert len(kf) == 1\n",
    "        return [(file, label) for file in kf[0].split(\"<>\")]\n",
    "\n",
    "def find_best_epoch(pth):\n",
    "    '''\n",
    "    Priority is given to high AUC > high epoch counts.\n",
    "    '''\n",
    "    ls_aucs = []\n",
    "\n",
    "    for epoch in pth.glob(\"Epoch */test-records.csv\"):\n",
    "        df_test_records = pd.read_csv(epoch)\n",
    "        ls_aucs.append((\n",
    "            int(epoch.parent.name.replace(\"Epoch \", \"\")),\n",
    "            roc_auc_score(df_test_records[\"actual\"], df_test_records[\"pred\"])\n",
    "        ))\n",
    "\n",
    "    return max(ls_aucs[::-1], key = lambda x: x[1])\n",
    "\n",
    "def method_name_to_func(method_name):\n",
    "    if method_name == \"atchley\":\n",
    "        from embed.physicochemical import atchley\n",
    "        return atchley()\n",
    "        \n",
    "    elif method_name == \"kidera\":\n",
    "        from embed.physicochemical import kidera\n",
    "        return kidera()\n",
    "        \n",
    "    elif method_name == \"rand\":\n",
    "        from embed.physicochemical import rand\n",
    "        return rand()\n",
    "        \n",
    "    elif method_name == \"aaprop\":\n",
    "        from embed.physicochemical import aaprop\n",
    "        return aaprop()\n",
    "        \n",
    "    elif method_name == \"tcrbert\":\n",
    "        from embed.llm import tcrbert\n",
    "        return tcrbert()\n",
    "        \n",
    "    elif method_name == \"sceptr-tiny\":\n",
    "        from sceptr import variant\n",
    "        return variant.tiny()\n",
    "        \n",
    "    elif method_name == \"sceptr-default\":\n",
    "        from sceptr import variant\n",
    "        return variant.default()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Cannot parse Method Name.\")\n",
    "\n",
    "def find_method(pth):\n",
    "    if \"autoencoder\" in pth.parent.name:\n",
    "        encoding_method_str = re.sub(r\"-autoencoder(-\\d*)?\", \"\", pth.parent.name)\n",
    "        encoding_method = method_name_to_func(encoding_method_str)\n",
    "        reduction_method = reducer.AutoEncoder(\n",
    "            encoding_method, encoding_method_str\n",
    "        )\n",
    "\n",
    "    elif \"johnson-lindenstarauss\" in pth.parent.name:\n",
    "        encoding_method_str = re.sub(r\"-johnson-lindenstarauss(-\\d*)?\", \"\", pth.parent.name)\n",
    "        encoding_method = method_name_to_func(encoding_method_str)\n",
    "        reduction_method = reducer.JohnsonLindenstarauss(\n",
    "            encoding_method.calc_vector_representations(\n",
    "                pd.read_csv(src / \"data/sample.tsv\", sep = \"\\t\", dtype = str)\n",
    "            ).shape[1]\n",
    "        )\n",
    "\n",
    "    elif \"no-reduction\" in pth.parent.name:\n",
    "        encoding_method_str = re.sub(r\"-no-reduction(-\\d*)?\", \"\", pth.parent.name)\n",
    "        encoding_method = method_name_to_func(encoding_method_str)\n",
    "        reduction_method = reducer.NoReduce()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Cannot parse method.\")\n",
    "    \n",
    "    return (encoding_method, reduction_method)\n",
    "\n",
    "def load_trained_model(model_encoding, model_reducer, best_epoch):\n",
    "    if isinstance(model_reducer, reducer.NoReduce):\n",
    "        model_trained = model.ordinary_classifier(model_encoding, use_cuda)\n",
    "    else:\n",
    "        model_trained = model.reduced_classifier(use_cuda)\n",
    "    \n",
    "    model_trained.load_state_dict(\n",
    "        torch.load(method_kf / f\"Epoch {best_epoch}/classifier.pth\")\n",
    "    )\n",
    "\n",
    "    for param in model_trained.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return model_trained\n",
    "\n",
    "def get_result(pth_positive_file, model_encoding, model_reducer, model_trained):\n",
    "    df = pd.read_csv(src / pth_positive_file, sep=\"\\t\", dtype=str)\n",
    "\n",
    "    # Embedding File\n",
    "    tensor_embeddings = model_encoding.calc_vector_representations(df)\n",
    "    \n",
    "    # Reducing Dimensionality\n",
    "    tensor_embeddings = model_reducer.reduce(tensor_embeddings)\n",
    "    \n",
    "    # Tensoring\n",
    "    tensor_embeddings = torch.from_numpy(tensor_embeddings).to(torch.float32)\n",
    "    tensor_embeddings = tensor_embeddings.cuda() if torch.cuda.is_available() and use_cuda else tensor_embeddings\n",
    "    \n",
    "    # Creating Prediction\n",
    "    predicted_label = model_trained(tensor_embeddings)\n",
    "\n",
    "    # Non-Zero Weights\n",
    "    nonzero_idx = torch.nonzero(model_trained.last_weights)[:, 0]\n",
    "    ls_ws = model_trained.last_weights[nonzero_idx][:, 0].tolist()\n",
    "    df = df.iloc[nonzero_idx.tolist()]\n",
    "    df[\"assigned_weights\"] = ls_ws\n",
    "\n",
    "    return df, predicted_label\n",
    "\n",
    "def make_interpretability_dir():\n",
    "    try:\n",
    "        os.makedirs(method_kf / \"interpretability\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ls_method_kf_dirs = list((src / data_src).glob(\"**/kfold-*\"))\n",
    "\n",
    "for idx, method_kf in enumerate(ls_method_kf_dirs):\n",
    "    ls_positive = read_kfold_set(method_kf / \"pos0-kfold.txt\", 1)\n",
    "    ls_negative = read_kfold_set(method_kf / \"neg0-kfold.txt\", 0)\n",
    "    ls_kfs = ls_positive + ls_negative\n",
    "    best_epoch, best_epoch_auc = find_best_epoch(method_kf)\n",
    "    model_encoding, model_reducer = find_method(method_kf)\n",
    "    model_trained = load_trained_model(model_encoding, model_reducer, best_epoch)\n",
    "    make_interpretability_dir()\n",
    "    \n",
    "    if (method_kf / \"interpretability/results_log.csv\").exists() and (method_kf / \"interpretability/used_model.txt\").exists():\n",
    "        continue\n",
    "\n",
    "    df_repertoire_with_label = pd.DataFrame({\"filenames\": [], \"true\": [], \"prediction\": []})\n",
    "    for pth_file, label in tqdm(ls_kfs, \n",
    "                                desc = str(method_kf.relative_to(Path.cwd() / \"results\"))+f\"; Progress: {idx+1}/{len(ls_method_kf_dirs)}\"):\n",
    "        df, pred = get_result(pth_file, model_encoding, model_reducer, model_trained)\n",
    "        df_repertoire_with_label = pd.concat([\n",
    "            df_repertoire_with_label,\n",
    "            pd.DataFrame({\"filenames\": [pth_file], \"true\": [label], \"prediction\": [pred.item()]})\n",
    "        ])\n",
    "        df.to_parquet(\n",
    "            method_kf / \"interpretability\" / Path(pth_file).name.replace(\".tsv\", \".pq\")\n",
    "        )\n",
    "\n",
    "    df_repertoire_with_label.set_index(\"filenames\").to_csv(method_kf / \"interpretability/results_log.csv\")\n",
    "\n",
    "    with open(method_kf / \"interpretability/used_model.txt\", \"w\") as f:\n",
    "        f.write(f\"Used Epoch: {best_epoch} with AUC {best_epoch_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('complete/sceptr-default-autoencoder-0/kfold-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_kf.relative_to(Path.cwd() / \"results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
